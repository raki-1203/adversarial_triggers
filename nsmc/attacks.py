import torch
import torch.nn.functional as F


"""
Contains different methods for attacking models. In particular, given the gradients for token
embeddings, it computes the optimal token replacements. This code runs on CPU.
"""


def hotflip_attack(averaged_grad, embedding_matrix, trigger_token_ids,
                   increase_loss=False, num_candidates=1):
    """
    The "Hotflip" attack described in Equation (2) of the paper. This code is heavily inspired by
    the nice code of Paul Michel here https://github.com/pmichel31415/translate/blob/paul/
    pytorch_translate/research/adversarial/adversaries/brute_force_adversary.py

    This function takes in the model's average_grad over a batch of examples, the model's
    token embedding matrix, and the current trigger token IDs. It returns the top token
    candidates for each position.

    If increase_loss=True, then the attack reverses the sign of the gradient and tries to increase
    the loss (decrease the model's probability of the true class). For targeted attacks, you want
    to decrease the loss of the target class (increase_loss=False).
    """
    averaged_grad = averaged_grad.cpu()
    embedding_matrix = embedding_matrix.cpu()
    trigger_token_embeds = torch.nn.functional.embedding(torch.LongTensor(trigger_token_ids),
                                                         embedding_matrix).detach().unsqueeze(0)
    averaged_grad = averaged_grad.unsqueeze(0)
    gradient_dot_embedding_matrix = torch.einsum("bij,kj->bik",
                                                 (averaged_grad, embedding_matrix))
    if not increase_loss:
        gradient_dot_embedding_matrix *= -1  # lower versus increase the class probability.
    if num_candidates > 1:  # get top k options
        _, best_k_ids = torch.topk(gradient_dot_embedding_matrix, num_candidates, dim=2)
        return best_k_ids.detach().cpu().numpy()[0]
    _, best_at_each_step = gradient_dot_embedding_matrix.max(2)
    return best_at_each_step[0].detach().cpu().numpy()


def hotflip_attack_modified(averaged_grad, embedding_matrix, trigger_token_ids,
                            increase_loss=False, num_candidates=1):
    """
    The "Hotflip" attack described in Equation (2) of the paper. This code is heavily inspired by
    the nice code of Paul Michel here https://github.com/pmichel31415/translate/blob/paul/
    pytorch_translate/research/adversarial/adversaries/brute_force_adversary.py

    This function takes in the model's average_grad over a batch of examples, the model's
    token embedding matrix, and the current trigger token IDs. It returns the top token
    candidates for each position.

    If increase_loss=True, then the attack reverses the sign of the gradient and tries to increase
    the loss (decrease the model's probability of the true class). For targeted attacks, you want
    to decrease the loss of the target class (increase_loss=False).
    """
    averaged_grad = averaged_grad.cpu()
    embedding_matrix = embedding_matrix.cpu()
    trigger_token_embeds = F.embedding(torch.LongTensor(trigger_token_ids),
                                       embedding_matrix).detach().unsqueeze(0)
    embedding_matrix = embedding_matrix.unsqueeze(0)
    embedding_matrix = torch.cat([embedding_matrix, embedding_matrix, embedding_matrix], dim=0)
    trigger_token_embeds = torch.transpose(trigger_token_embeds, 0, 1)

    averaged_grad = averaged_grad.unsqueeze(0)
    averaged_grad = torch.transpose(averaged_grad, 0, 1)
    gradient_dot_embedding_matrix = torch.einsum("bij,bkj->bik",
                                                 (averaged_grad, embedding_matrix - trigger_token_embeds))
    gradient_dot_embedding_matrix = torch.transpose(gradient_dot_embedding_matrix, 0, 1)
    if not increase_loss:
        gradient_dot_embedding_matrix *= -1  # lower versus increase the class probability.
    if num_candidates > 1:  # get top k options
        _, best_k_ids = torch.topk(gradient_dot_embedding_matrix, num_candidates, dim=2)
        return best_k_ids.detach().cpu().numpy()[0]
    _, best_at_each_step = gradient_dot_embedding_matrix.max(2)
    return best_at_each_step[0].detach().cpu().numpy()


def hotflip_attack_without_sentiment_token(averaged_grad, embedding_matrix, trigger_token_ids,
                                           increase_loss=False, num_candidates=1):
    """
    The "Hotflip" attack described in Equation (2) of the paper. This code is heavily inspired by
    the nice code of Paul Michel here https://github.com/pmichel31415/translate/blob/paul/
    pytorch_translate/research/adversarial/adversaries/brute_force_adversary.py

    This function takes in the model's average_grad over a batch of examples, the model's
    token embedding matrix, and the current trigger token IDs. It returns the top token
    candidates for each position.

    If increase_loss=True, then the attack reverses the sign of the gradient and tries to increase
    the loss (decrease the model's probability of the true class). For targeted attacks, you want
    to decrease the loss of the target class (increase_loss=False).
    """
    averaged_grad = averaged_grad.cpu()
    embedding_matrix = embedding_matrix.cpu()
    trigger_token_embeds = F.embedding(torch.LongTensor(trigger_token_ids),
                                       embedding_matrix).detach().unsqueeze(0)
    embedding_matrix = embedding_matrix.unsqueeze(0)
    embedding_matrix = torch.cat([embedding_matrix, embedding_matrix, embedding_matrix], dim=0)
    trigger_token_embeds = torch.transpose(trigger_token_embeds, 0, 1)

    averaged_grad = averaged_grad.unsqueeze(0)
    averaged_grad = torch.transpose(averaged_grad, 0, 1)
    gradient_dot_embedding_matrix = torch.einsum("bij,bkj->bik",
                                                 (averaged_grad, embedding_matrix - trigger_token_embeds))
    gradient_dot_embedding_matrix = torch.transpose(gradient_dot_embedding_matrix, 0, 1)
    if not increase_loss:
        gradient_dot_embedding_matrix *= -1  # lower versus increase the class probability.
    if num_candidates > 1:  # get top k options
        _, best_k_ids = torch.topk(gradient_dot_embedding_matrix, num_candidates, dim=2)
        return best_k_ids.detach().cpu().numpy()[0]
    _, best_at_each_step = gradient_dot_embedding_matrix.max(2)
    return best_at_each_step[0].detach().cpu().numpy()

